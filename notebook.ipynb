{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2d7e5eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cff8d98",
   "metadata": {},
   "source": [
    "***Projected Subgradient Method***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5dbd5279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(verbose=False):\n",
    "\n",
    "    data = pd.read_csv(\"all_stocks_5yr.csv\")\n",
    "    df = data[[\"date\", \"Name\", \"close\"]]\n",
    "\n",
    "    prices = df.pivot(index=\"date\", columns=\"Name\", values=\"close\")\n",
    "    prices = prices.dropna(axis=1)\n",
    "\n",
    "    # with open(\"prices.txt\", \"w\") as f:\n",
    "    #     f.write(prices.to_string())\n",
    "    #     f.close()\n",
    "\n",
    "    returns = prices.pct_change().dropna()\n",
    "    # with open(\"rendements.txt\", \"w\") as f:\n",
    "    #     f.write(returns.to_string())\n",
    "    #     f.close()\n",
    "\n",
    "\n",
    "    X = returns.values\n",
    "    mu = returns.mean().values\n",
    "    Sigma = returns.cov().values\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(\"X shape =\", X.shape)\n",
    "        print(\"mu shape =\", mu.shape)\n",
    "        print(\"Sigma shape =\", Sigma.shape)\n",
    "    \n",
    "    return X, mu, Sigma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "66fbb7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection_on_simplex(v):\n",
    "    \"\"\"\n",
    "    Project a vector v onto the simplex:\n",
    "        Δ = { w >= 0, sum(w) = 1 }\n",
    "    \"\"\"\n",
    "   \n",
    "    u = sorted(v, reverse=True)\n",
    "    cumulative_sum = np.cumsum(u)\n",
    "\n",
    "    thetas = (cumulative_sum - 1) / (np.arange(1, len(v) + 1))\n",
    "    k = np.where(u - thetas > 0)[0].max()\n",
    "\n",
    "    theta = (cumulative_sum[k] - 1) / (k + 1)\n",
    "    w = np.maximum(v - theta, 0)\n",
    "\n",
    "    return w\n",
    "\n",
    "def test(): \n",
    "    v = np.array([0.9, 0.9, 0.2])\n",
    "    v2 = -1 * v\n",
    "\n",
    "    print(\"w1 : \", projection_on_simplex(v))\n",
    "    print(\"w2 : \", projection_on_simplex(v2))\n",
    "\n",
    "#test()\n",
    "\n",
    "def sign(v):\n",
    "    result = np.zeros_like(v)\n",
    "\n",
    "    result[v > 0] = 1\n",
    "    result[v < 0] = -1\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def obj_f_nonSmooth(w, wprev, Sigma, mu, lam, c=0.0):\n",
    "\n",
    "    return 0.5 * w.T @ Sigma @ w -lam * (mu @ w) + \\\n",
    "            c * np.sum(np.abs(w - wprev))\n",
    "\n",
    "def projected_subgradient_method(lam, c, Sigma, mu, wprev, alpha0=1, max_iter=50000, tol= 1e-8, plot=False): \n",
    "\n",
    "    w = wprev\n",
    "    k = 0\n",
    "    if plot:\n",
    "        obj_values = []\n",
    "    for i in tqdm(range(max_iter)):\n",
    "\n",
    "        if plot:\n",
    "            obj_values.append(obj_f_nonSmooth(w, wprev, Sigma, mu, lam, c))\n",
    "\n",
    "        g = Sigma @ w - lam * mu + c * np.sign(w - wprev)\n",
    "        step_size = alpha0 / (k + 1)\n",
    "        w_new = projection_on_simplex(w - step_size * g)\n",
    "        \n",
    "        if (np.linalg.norm(w_new - w) < tol):\n",
    "            print(f\"Converged in {i} iterations.\")\n",
    "            break\n",
    "\n",
    "        w = w_new\n",
    "\n",
    "\n",
    "    if plot:\n",
    "        plt.plot(obj_values)\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        \n",
    "        plt.yscale(\"log\")\n",
    "        plt.ylabel(\"Objective value (log scale)\")\n",
    "        plt.title(\"Convergence of Projected Subgradient Method\")\n",
    "        plt.show()\n",
    "    \n",
    "    return w\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca755e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape = (1258, 470)\n",
      "mu shape = (470,)\n",
      "Sigma shape = (470, 470)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:12<00:00, 4165.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal weights: [0.00427979 0.         0.00429547 0.         0.         0.\n",
      " 0.00429645 0.         0.         0.00429603 0.00428698 0.\n",
      " 0.00428781 0.         0.         0.         0.00425647 0.\n",
      " 0.         0.         0.00428512 0.00429723 0.         0.\n",
      " 0.00429936 0.00429716 0.         0.         0.         0.00426542\n",
      " 0.         0.         0.00429652 0.00424836 0.         0.00430148\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00420655 0.004229   0.00430173 0.         0.\n",
      " 0.0042527  0.         0.00429323 0.         0.         0.\n",
      " 0.00429172 0.         0.         0.         0.         0.00427653\n",
      " 0.00429725 0.         0.         0.00422322 0.         0.\n",
      " 0.00429947 0.00429904 0.         0.         0.         0.00426934\n",
      " 0.00427344 0.00428104 0.00427891 0.00428315 0.00428829 0.00429488\n",
      " 0.         0.00428122 0.         0.00427353 0.         0.00429713\n",
      " 0.         0.00429577 0.00428567 0.00426556 0.         0.00414876\n",
      " 0.         0.         0.         0.00429859 0.00429155 0.\n",
      " 0.         0.         0.         0.00426936 0.00428182 0.\n",
      " 0.         0.00428851 0.00429148 0.00426249 0.         0.\n",
      " 0.00426123 0.         0.00429012 0.         0.         0.\n",
      " 0.         0.00420606 0.         0.00426976 0.00429681 0.00426383\n",
      " 0.00429461 0.         0.         0.         0.00430178 0.\n",
      " 0.         0.00429823 0.         0.00418406 0.00418939 0.00427642\n",
      " 0.         0.         0.00427967 0.         0.00429733 0.\n",
      " 0.         0.00429342 0.00429019 0.00422263 0.         0.00425953\n",
      " 0.         0.         0.         0.         0.         0.00427435\n",
      " 0.00426806 0.00429715 0.         0.00427816 0.00425518 0.00429145\n",
      " 0.         0.         0.00427226 0.00429346 0.         0.00429351\n",
      " 0.00429391 0.         0.         0.00423689 0.00426137 0.\n",
      " 0.00429512 0.00420842 0.         0.00426287 0.00428265 0.\n",
      " 0.         0.00429908 0.00430324 0.         0.00424165 0.00423287\n",
      " 0.00427972 0.0042743  0.00422566 0.         0.00423101 0.00427551\n",
      " 0.         0.00429242 0.         0.00428573 0.         0.00428624\n",
      " 0.         0.00427414 0.00430112 0.00428545 0.         0.00428305\n",
      " 0.0042711  0.         0.         0.         0.         0.0042722\n",
      " 0.00422661 0.         0.0042262  0.         0.         0.00424989\n",
      " 0.         0.         0.00426988 0.00427105 0.         0.\n",
      " 0.         0.00425962 0.00429275 0.         0.00424585 0.\n",
      " 0.         0.         0.         0.         0.         0.00427688\n",
      " 0.00429726 0.00428932 0.00426819 0.         0.         0.\n",
      " 0.00424338 0.         0.00427242 0.00427538 0.         0.0042777\n",
      " 0.         0.00426089 0.00428409 0.         0.00423742 0.\n",
      " 0.00429    0.00420215 0.00428665 0.00429001 0.00429478 0.\n",
      " 0.0043071  0.00425636 0.00426721 0.00427428 0.00428637 0.00428086\n",
      " 0.         0.00429596 0.         0.00429627 0.         0.\n",
      " 0.         0.         0.         0.00424364 0.         0.00429447\n",
      " 0.00423457 0.         0.         0.0042666  0.         0.\n",
      " 0.00421405 0.         0.         0.00429544 0.         0.00429497\n",
      " 0.         0.00425968 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.00428216 0.00419919\n",
      " 0.00428742 0.00429389 0.00420945 0.00429901 0.         0.\n",
      " 0.00430024 0.         0.         0.00428568 0.00420143 0.\n",
      " 0.         0.         0.00430439 0.         0.00426208 0.00428869\n",
      " 0.         0.00427418 0.         0.00421479 0.0042799  0.\n",
      " 0.00429784 0.00429759 0.00427428 0.         0.00428305 0.00427462\n",
      " 0.00428425 0.         0.00424706 0.         0.00429484 0.00427406\n",
      " 0.0042783  0.         0.00425626 0.         0.         0.00428785\n",
      " 0.00429114 0.00428405 0.         0.00429928 0.00427844 0.\n",
      " 0.         0.00429195 0.00428086 0.         0.00427101 0.\n",
      " 0.00428713 0.00427957 0.00425904 0.00428787 0.00428736 0.00428203\n",
      " 0.00427904 0.00426879 0.00428659 0.00428789 0.00426089 0.\n",
      " 0.         0.00427892 0.         0.         0.00428146 0.\n",
      " 0.00429483 0.00422733 0.         0.         0.         0.\n",
      " 0.00415608 0.         0.         0.         0.         0.0042631\n",
      " 0.         0.         0.         0.00425688 0.00429736 0.00425489\n",
      " 0.00427032 0.         0.00429883 0.         0.00428919 0.00426816\n",
      " 0.         0.00424828 0.         0.         0.00428826 0.00428307\n",
      " 0.         0.         0.         0.         0.0042813  0.\n",
      " 0.00428272 0.         0.         0.         0.00428967 0.00429328\n",
      " 0.         0.         0.         0.00426912 0.00425106 0.00427596\n",
      " 0.         0.00427845 0.         0.         0.         0.\n",
      " 0.00429401 0.00426594 0.         0.         0.         0.\n",
      " 0.         0.00430419 0.         0.00428934 0.         0.00429473\n",
      " 0.00428918 0.         0.00429907 0.         0.00421182 0.\n",
      " 0.         0.00424825 0.         0.         0.         0.00426234\n",
      " 0.00428856 0.         0.         0.00428911 0.         0.00429134\n",
      " 0.00428493 0.         0.00424698 0.         0.00427731 0.00426616\n",
      " 0.         0.00428743 0.00429665 0.         0.00429847 0.00429977\n",
      " 0.0042564  0.00428681 0.00424991 0.         0.00427785 0.00429828\n",
      " 0.         0.        ]\n",
      "Sum of weights: 1.000000000000142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X, mu, Sigma = read_data(verbose=True)\n",
    "lam = 0.1\n",
    "c = 1\n",
    "wprev = np.ones(X.shape[1]) / X.shape[1]\n",
    "\n",
    "w = projected_subgradient_method(lam, c, Sigma, mu, wprev, tol= 1e-8, plot=True)\n",
    "print(\"Optimal weights:\", w)\n",
    "print(\"Sum of weights:\", np.sum(w))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
